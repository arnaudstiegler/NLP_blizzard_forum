{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on the posts\n",
    "\n",
    "We read the file and use google NLP features to get a sentiment score on the document, and for each word within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data/1000_posts.json')\n",
    "\n",
    "entities_result = []\n",
    "annotations_result = []\n",
    "counter = 0\n",
    "for index, row in df.iloc[0:100,:].iterrows():\n",
    "    texts = [row['init_post']] + row['comments']\n",
    "    for text in texts:\n",
    "        document = types.Document(\n",
    "                    content=text,\n",
    "                    type=enums.Document.Type.PLAIN_TEXT,\n",
    "                    language='en')\n",
    "        entities = client.analyze_entity_sentiment(document).entities\n",
    "        entities_result.append(entities)\n",
    "        \n",
    "        annotations = client.analyze_sentiment(document=document)\n",
    "        annotations_result.append({\"score\": annotations.document_sentiment.score,\n",
    "               \"magnitude\": annotations.document_sentiment.magnitude})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aggregate the results for each word to get overall values for each word for the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docword_sentiment = {}\n",
    "for i in range(len(entities_result)):\n",
    "    for j in range(len(entities_result[i])):\n",
    "        if(entities_result[i][j].name not in processed_docword_sentiment.keys()):\n",
    "            processed_docword_sentiment[entities_result[i][j].name] = {'score':[entities_result[i][j].sentiment.score],\n",
    "                                                             'magnitude':[entities_result[i][j].sentiment.magnitude]}\n",
    "        else:\n",
    "            processed_docword_sentiment[entities_result[i][j].name]['score'].append(entities_result[i][j].sentiment.score)\n",
    "            processed_docword_sentiment[entities_result[i][j].name]['magnitude'].append(entities_result[i][j].sentiment.magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/document_entity_sentiment.json', 'w') as outfile:\n",
    "    json.dump(processed_docword_sentiment, outfile, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open('data/document_sentiment.json', 'w') as outfile:\n",
    "    json.dump(annotations_result, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': [-0.20000000298023224,\n",
       "  0.30000001192092896,\n",
       "  -0.30000001192092896,\n",
       "  0.5,\n",
       "  -0.6000000238418579,\n",
       "  -0.10000000149011612,\n",
       "  0.4000000059604645,\n",
       "  0.0,\n",
       "  0.800000011920929,\n",
       "  0.5,\n",
       "  0.10000000149011612,\n",
       "  0.5,\n",
       "  0.800000011920929,\n",
       "  0.5,\n",
       "  0.8999999761581421,\n",
       "  0.6000000238418579,\n",
       "  0.8999999761581421,\n",
       "  0.6000000238418579,\n",
       "  0.10000000149011612,\n",
       "  0.10000000149011612,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.699999988079071,\n",
       "  0.0,\n",
       "  0.20000000298023224,\n",
       "  0.10000000149011612,\n",
       "  0.8999999761581421,\n",
       "  -0.10000000149011612,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4000000059604645,\n",
       "  0.10000000149011612,\n",
       "  0.20000000298023224,\n",
       "  -0.699999988079071,\n",
       "  0.10000000149011612,\n",
       "  0.10000000149011612,\n",
       "  0.800000011920929,\n",
       "  -0.10000000149011612,\n",
       "  0.800000011920929,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.20000000298023224,\n",
       "  0.20000000298023224,\n",
       "  0.0,\n",
       "  0.8999999761581421,\n",
       "  0.8999999761581421,\n",
       "  0.699999988079071,\n",
       "  -0.800000011920929,\n",
       "  0.0,\n",
       "  0.5,\n",
       "  0.30000001192092896,\n",
       "  -0.10000000149011612,\n",
       "  0.20000000298023224,\n",
       "  0.6000000238418579,\n",
       "  0.6000000238418579,\n",
       "  0.6000000238418579,\n",
       "  0.20000000298023224,\n",
       "  0.5,\n",
       "  -0.4000000059604645,\n",
       "  0.0,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.30000001192092896,\n",
       "  0.20000000298023224,\n",
       "  0.4000000059604645,\n",
       "  0.6000000238418579,\n",
       "  0.800000011920929,\n",
       "  0.8999999761581421],\n",
       " 'magnitude': [0.20000000298023224,\n",
       "  0.30000001192092896,\n",
       "  0.30000001192092896,\n",
       "  3.5,\n",
       "  1.2999999523162842,\n",
       "  0.10000000149011612,\n",
       "  0.4000000059604645,\n",
       "  0.699999988079071,\n",
       "  0.800000011920929,\n",
       "  1.600000023841858,\n",
       "  0.10000000149011612,\n",
       "  0.5,\n",
       "  0.800000011920929,\n",
       "  0.5,\n",
       "  1.7999999523162842,\n",
       "  1.899999976158142,\n",
       "  0.8999999761581421,\n",
       "  0.6000000238418579,\n",
       "  0.10000000149011612,\n",
       "  0.10000000149011612,\n",
       "  0.699999988079071,\n",
       "  1.2999999523162842,\n",
       "  1.5,\n",
       "  0.0,\n",
       "  1.600000023841858,\n",
       "  0.10000000149011612,\n",
       "  0.8999999761581421,\n",
       "  0.10000000149011612,\n",
       "  0.0,\n",
       "  1.5,\n",
       "  0.4000000059604645,\n",
       "  0.10000000149011612,\n",
       "  0.20000000298023224,\n",
       "  0.699999988079071,\n",
       "  1.5,\n",
       "  1.5,\n",
       "  0.800000011920929,\n",
       "  0.10000000149011612,\n",
       "  3.4000000953674316,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.4000000059604645,\n",
       "  0.20000000298023224,\n",
       "  1.7000000476837158,\n",
       "  0.8999999761581421,\n",
       "  1.899999976158142,\n",
       "  2.799999952316284,\n",
       "  0.800000011920929,\n",
       "  1.399999976158142,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.10000000149011612,\n",
       "  0.20000000298023224,\n",
       "  0.6000000238418579,\n",
       "  1.2000000476837158,\n",
       "  1.2000000476837158,\n",
       "  0.20000000298023224,\n",
       "  1.100000023841858,\n",
       "  0.4000000059604645,\n",
       "  0.0,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.699999988079071,\n",
       "  0.6000000238418579,\n",
       "  0.30000001192092896,\n",
       "  0.5,\n",
       "  0.4000000059604645,\n",
       "  0.6000000238418579,\n",
       "  0.800000011920929,\n",
       "  0.8999999761581421]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data['Classic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by computing the average sentiment score for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment score over the posts is -0.067428\n"
     ]
    }
   ],
   "source": [
    "score_sum = 0\n",
    "for item in doc_data:\n",
    "    score_sum += item['score']*item['magnitude']\n",
    "    \n",
    "print(\"Average sentiment score over the posts is {:2f}\".format(score_sum/len(doc_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average sentiment score is slightly below 0, which indicates that the average sentiment is rather negative. However, the value is still very close to 0 which indicates mixed feelings among the posts (which corresponds to the fact that although people do complain about the game, they are still trying to improve it by providing feedback and are not only writing to express dissatisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aggregated_score = []\n",
    "for item in word_data.keys():\n",
    "    #We only account for words that can be found in at least 50 posts\n",
    "    if(len(word_data[item]['score']) > 50):\n",
    "        aggregated_score.append((item, sum(np.array(word_data[item]['score'])*np.array(word_data[item]['magnitude']))/len(word_data[item]['score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 most positive words are:\n",
      "[('fun', 0.528315775651681), ('Classic', 0.37642857332314783), ('zones', 0.13692307485697375), ('example', 0.10652173530796306), ('class', 0.0950000011435513), ('power', 0.07753246661130486), ('games', 0.06279569776788839), ('life', 0.06215384385677486), ('ability', 0.05508196753556618), ('gear', 0.044363634153523235)]\n",
      "The top-10 most negative words are:\n",
      "[('problem', -0.42785714148410725), ('damage', -0.36709302032582997), ('issues', -0.3392727232683791), ('nothing', -0.2863398719466981), ('issue', -0.2135294121942101), ('People', -0.16950980554754835), ('Alliance', -0.15937500143423686), ('post', -0.1508333312205616), ('Blizzard', -0.14409836148873698), ('blizzard', -0.13682539564039978)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The top-10 most positive words are:\")\n",
    "print(sorted(aggregated_score, key=lambda tup: -tup[1])[0:10])\n",
    "print(\"The top-10 most negative words are:\")\n",
    "print(sorted(aggregated_score, key=lambda tup: tup[1])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are interesting: \"Classic\" and 'classic' are among the top-10 most positive words within the posts which shows the enthusiasm of the players for the newly launched Classic WOW game. On the contrary, Blizzard has a very low score (you can actually find the occurence of Blizzard twice in the bottom 10 words). \n",
    "\n",
    "Besides, it is interesting to note that 'zones', 'gear', 'class' have very high scores as well, which might indicte that those elements are the most-liked features of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forum_scraping]",
   "language": "python",
   "name": "conda-env-forum_scraping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
