{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on the posts\n",
    "\n",
    "We read the file and use google NLP features to get a sentiment score on the document, and for each word within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data/1000_posts.json')\n",
    "\n",
    "entities_result = []\n",
    "annotations_result = []\n",
    "counter = 0\n",
    "for index, row in df.iloc[0:100,:].iterrows():\n",
    "    texts = [row['init_post']] + row['comments']\n",
    "    for text in texts:\n",
    "        document = types.Document(\n",
    "                    content=text.lower(),\n",
    "                    type=enums.Document.Type.PLAIN_TEXT,\n",
    "                    language='en')\n",
    "        entities = client.analyze_entity_sentiment(document).entities\n",
    "        entities_result.append(entities)\n",
    "        \n",
    "        annotations = client.analyze_sentiment(document=document)\n",
    "        annotations_result.append({\"score\": annotations.document_sentiment.score,\n",
    "               \"magnitude\": annotations.document_sentiment.magnitude})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aggregate the results for each word to get overall values for each word for the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docword_sentiment = {}\n",
    "for i in range(len(entities_result)):\n",
    "    for j in range(len(entities_result[i])):\n",
    "        if(entities_result[i][j].name not in processed_docword_sentiment.keys()):\n",
    "            processed_docword_sentiment[entities_result[i][j].name] = {'score':[entities_result[i][j].sentiment.score],\n",
    "                                                             'magnitude':[entities_result[i][j].sentiment.magnitude]}\n",
    "        else:\n",
    "            processed_docword_sentiment[entities_result[i][j].name]['score'].append(entities_result[i][j].sentiment.score)\n",
    "            processed_docword_sentiment[entities_result[i][j].name]['magnitude'].append(entities_result[i][j].sentiment.magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/document_entity_sentiment.json', 'w') as outfile:\n",
    "    json.dump(processed_docword_sentiment, outfile, indent=4, ensure_ascii=False)\n",
    "    \n",
    "with open('data/document_sentiment.json', 'w') as outfile:\n",
    "    json.dump(annotations_result, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = processed_docword_sentiment\n",
    "doc_data = annotations_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by computing the average sentiment score for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment score over the posts is -0.078245\n"
     ]
    }
   ],
   "source": [
    "score_sum = 0\n",
    "for item in doc_data:\n",
    "    score_sum += item['score']*item['magnitude']\n",
    "    \n",
    "print(\"Average sentiment score over the posts is {:2f}\".format(score_sum/len(doc_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average sentiment score is slightly below 0, which indicates that the average sentiment is rather negative. However, the value is still very close to 0 which indicates mixed feelings among the posts (which corresponds to the fact that although people do complain about the game, they are still trying to improve it by providing feedback and are not only writing to express dissatisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_score = []\n",
    "score_dict = {}\n",
    "for item in word_data.keys():\n",
    "    aggregated_score.append((item, sum(np.array(word_data[item]['score'])*np.array(word_data[item]['magnitude']))/len(word_data[item]['score'])))\n",
    "    score_dict[item] = sum(np.array(word_data[item]['score'])*np.array(word_data[item]['magnitude']))/len(word_data[item]['score'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08693181591277778"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict['classic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 most positive words are:\n",
      "[('gameplay type', 2.279999957680701), ('talent streamlining', 1.7099999332427984), ('motivator', 1.7099999332427984), ('forgiveness', 1.7099999332427984), (\"za'qul\", 1.7099999332427984), ('awesomeness', 1.6649999237060555), ('trolololol', 1.6199999141693127), ('though.', 1.6199999141693127), ('bagsnon boe', 1.6199999141693127), ('summation', 1.6199999141693127)]\n",
      "The top-10 most negative words are:\n",
      "[('johnny', -2.4299999785423267), ('class balance philosophy', -2.079999954700469), ('attack frenzy', -1.920000104904176), ('self torture', -1.7099999332427984), ('goblins lack', -1.7099999332427984), ('garbage content', -1.7099999332427984), ('reputation farming', -1.7099999332427984), ('pity bonus', -1.6199999141693127), ('lolmode', -1.6199999141693127), ('feed', -1.6199999141693127)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The top-10 most positive words are:\")\n",
    "print(sorted(aggregated_score, key=lambda tup: -tup[1])[0:10])\n",
    "print(\"The top-10 most negative words are:\")\n",
    "print(sorted(aggregated_score, key=lambda tup: tup[1])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-10 words are not that interesting. However, the bottom-10 are as we can see two features of the game that have been regularly debated: 'class balance philosophy' which is about balancing the power of each class of the game, and 'reputation farming' which is a very time-consuming part of the game currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for blizzard are:\n",
      "Score for blizzard is: -0.116\n",
      "Score for blizz is: -0.101\n",
      "\n",
      "\n",
      "The score for wow classic is:\n",
      "Score for classic is: 0.087\n",
      "\n",
      "\n",
      "The score for the expansions are:\n",
      "Score for bfa is: -0.022\n",
      "Score for legion is: -0.059\n",
      "Score for wod is: 0.022\n",
      "Score for cataclysm is: 0.029\n",
      "Score for mop is: 0.010\n",
      "Score for wotlk is: 0.052\n",
      "Score for tbc is: -0.017\n",
      "Score for vanilla is: -0.018\n",
      "\n",
      "\n",
      "The score for the game features are:\n",
      "Score for leveling is: 0.035\n",
      "Score for dungeon is: -0.004\n",
      "Score for raid is: -0.016\n",
      "Score for pvp is: -0.015\n",
      "Score for pve is: 0.035\n",
      "Score for azerite wq is: -0.180\n",
      "Score for azerite is: -0.230\n"
     ]
    }
   ],
   "source": [
    "names = ['blizzard','blizz']\n",
    "\n",
    "print(\"The scores for blizzard are:\")\n",
    "for item in names:\n",
    "    print(\"Score for {} is: {:.3f}\".format(item,score_dict[item]))\n",
    "    \n",
    "print('\\n')  \n",
    "classic = ['classic']\n",
    "\n",
    "print(\"The score for wow classic is:\")\n",
    "print(\"Score for {} is: {:.3f}\".format('classic',score_dict['classic']))\n",
    "print('\\n')\n",
    "\n",
    "expansions = ['bfa','legion','wod','cataclysm','mop','wotlk','tbc','vanilla']\n",
    "\n",
    "print(\"The score for the expansions are:\")\n",
    "for item in expansions:\n",
    "    print(\"Score for {} is: {:.3f}\".format(item,score_dict[item]))\n",
    "    \n",
    "print('\\n')\n",
    "game_features = ['leveling','dungeon','raid','pvp','pve',\n",
    "        'azerite wq', 'azerite']\n",
    "print(\"The score for the game features are:\")\n",
    "for item in game_features:\n",
    "    print(\"Score for {} is: {:.3f}\".format(item,score_dict[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blizzard**: it seems that blizzard is somewhat unpopular on those posts since 'blizzard' and 'blizz' have quite a negative sentiment score associated with their name.\n",
    "\n",
    "**Expansions**: it is interesting to see that the different WOW expansions have very different sentiment score. Classic wow has the highest sentiment score which seems consistent with the current success of the game. On the contrary, the current extension ('bfa') has quite a low sentiment score. There seems to be a pattern where the intermediary expansions have a positive sentiment score, and the oldest and most recent expansions have a negative sentiment score.\n",
    "\n",
    "**Game features**: 'pve' (which stands for player vs environment) and 'leveling' have a very high sentiment score and it is consistent with the fact that those are elements that have been quite positively judged by players. On the contrary, 'azerite' and 'azerite wq' have a very low sentiment score: they are very controversial aspects of the game (azerite is some sort of currency for progress within the game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forum_scraping]",
   "language": "python",
   "name": "conda-env-forum_scraping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
